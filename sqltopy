# Assuming 'df' is your DataFrame created earlier
average_prices_by_category = df.groupby('category')['price'].mean()

# Display the result
print(average_prices_by_category)

SELECT AVG(price), category
FROM Listings
GROUP BY category



---------------------------------------------------------------------------------------------------------------------------------
average_prices_by_city = df.groupby('city')['price'].mean()

# 3. Find the city with the lowest average price
city_with_lowest_avg_price = average_prices_by_city.idxmin()
lowest_avg_price_value     = average_prices_by_city.min()

# 4. Print the results in table form
print("city\tAVG(price)")
print(f"{city_with_lowest_avg_price}\t{int(lowest_avg_price_value)}")

 SELECT city, AVG(price)
FROM Listings
GROUP BY city
ORDER BY AVG(price) ASC
LIMIT 1


--------------------------------------------------------------------------------------------------------------------------------------
SELECT
  department_id,
  FLOOR(AVG(satisfaction_score)) AS avg_satisfaction_score
FROM employee_satisfaction
GROUP BY department_id;


# 1. Calculate the mean satisfaction score per department
avg_scores = df.groupby('department_id')['satisfaction_score'].mean()

# 2. Round down to the nearest whole number
avg_scores = np.floor(avg_scores).astype(int)



----------------------------------------------------------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np

# Example: load from CSV (or replace with your own data source)
# df = pd.read_csv("employee_satisfaction.csv")

# Ensure your DataFrame has the columns: 
#   - satisfaction_score (numeric)
#   - job_category_id
#   - department_id

# 1. Group by the two IDs and compute the average
grouped = (
    df
    .groupby(['job_category_id', 'department_id'], as_index=False)
    .agg(avg_score=('satisfaction_score', 'mean'))
)

# 2. Apply ceiling and floor
grouped['Ceil_Avg_Satisfaction']  = np.ceil(grouped['avg_score'])
grouped['Floor_Avg_Satisfaction'] = np.floor(grouped['avg_score'])

# 3. (Optional) Reorder or drop the intermediate avg_score column
result = grouped[
    ['Ceil_Avg_Satisfaction', 'Floor_Avg_Satisfaction', 
     'job_category_id', 'department_id']
]

print(result)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT campaign_id, SUM(conversions)
FROM ad_campaigns
WHERE start_date BETWEEN '01/04/2024' AND '30/04/2024'
GROUP BY campaign_id
ORDER BY SUM(conversions) DESC
LIMIT 5

top5 = (
    df[df["start_date"].between("2024-04-01", "2024-04-30")]
    .groupby("campaign_id", as_index=False)["conversions"]
    .sum()
    .rename(columns={"conversions": "total_conversions"})
    .sort_values("total_conversions", ascending=False)
    .head(5)
)

print(top5)

---------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT DISTINCT ad_type
FROM ad_campaigns
WHERE conversions > 100
  AND start_date >= '2024-05-01'
  AND start_date <  '2024-06-01';


import pandas as pd

# 1. Load the CSV and parse dates
df = pd.read_csv("ad_campaigns.csv", parse_dates=["start_date"])

# 2. Build a mask for May 2024 and conversions > 100
mask = (
    (df["start_date"] >= "2024-05-01") &
    (df["start_date"] <  "2024-06-01") &
    (df["conversions"] > 100)
)

# 3. Pull out the ad_type column, drop duplicates, and reset the index
result = (
    df.loc[mask, ["ad_type"]]
      .drop_duplicates()
      .reset_index(drop=True)
)

# 4. Display
print(result)


import pandas as pd

df = pd.read_csv("ad_campaigns.csv", parse_dates=["start_date"])

unique_ad_types = (
    df[
        df["start_date"].between("2024-05-01", "2024-05-31") &
        (df["conversions"] > 100)
    ]
    .loc[:, ["ad_type"]]
    .drop_duplicates()
    .reset_index(drop=True)
)

print(unique_ad_types)


-----------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT campaign_name , start_date, revenue
FROM ad_campaigns
WHERE start_date >= '2024-06-01'
AND start_date < '2024-07-01'
AND revenue = 0


import pandas as pd

# ensure your dates are datetimes
df["start_date"] = pd.to_datetime(df["start_date"])

# filter for June 2024 and zero revenue
revenue_zero = (
    df.loc[
        (df["start_date"] >= "2024-06-01") &
        (df["start_date"] <  "2024-07-01") &
        (df["revenue"] == 0),
        ["campaign_name", "start_date", "revenue"]
    ]
    .reset_index(drop=True)
)

print(revenue_zero)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT COUNT (DISTINCT artist_id)
FROM fct_artist_recommendations
WHERE recommendation_date >= '01/04/2024' AND recommendation_date < '01/05/2024'

count = (
    df.loc[
        (df["recommendation_date"] >= "2024-04-01") &
        (df["recommendation_date"] <  "2024-05-01"),
        "artist_id"
    ]
    .nunique())
print(count)

------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT COUNT(recommendation_id)
FROM fct_artist_recommendations
WHERE recommendation_date >= '01/05/2024' AND recommendation_date < '01/06/2024'
AND is_new_artist = 1


count = (
df.loc[
(df["recommendation_date"] >= "2024-05-01") &
(df["recommendation_date"] < "2024-06-01") &
(df["is_new_artist"] == 1), "recommendation_id"].count())
print(count)


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT 
  strftime('%m', recommendation_date) AS month,
  COUNT(DISTINCT artist_id) AS new_artist_count
FROM fct_artist_recommendations
WHERE 
  is_new_artist = 1
  AND recommendation_date >= '2024-04-01'
  AND recommendation_date < '2024-07-01'
GROUP BY month
ORDER BY month;



import pandas as pd

# Ensure recommendation_date is in datetime format
df["recommendation_date"] = pd.to_datetime(df["recommendation_date"])

# Filter for Q2 2024 and only new artists
filtered = df[
    (df["recommendation_date"] >= "2024-04-01") &
    (df["recommendation_date"] < "2024-07-01") &
    (df["is_new_artist"] == 1)
].copy()

# Extract month for grouping
filtered["month"] = filtered["recommendation_date"].dt.strftime("%m")

# Count distinct artist_id per month
result = (
    filtered.groupby("month", as_index=False)["artist_id"]
    .nunique()
    .rename(columns={"artist_id": "new_artist_count"})
    .sort_values("month")
)

print(result)

--------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT MIN(response_time_hours)
FROM fct_guest_inquiries
WHERE inquiry_date BETWEEN '2024-01-01' AND '2024-01-31'

filtered = df[
    (df["inquiry_date"] >= "2024-01-01") &
    (df["inquiry_date"] < "2024-02-01")
].copy()

result = filtered["response_time_hours"].min()


----------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT 
  ROUND(AVG(response_time_hours)) AS avg_response_time_hours
FROM 
  fct_guest_inquiries
WHERE 
  inquiry_date >= '2024-01-01' 
  AND inquiry_date < '2024-02-01';


filtered = df[
    (df["inquiry_date"] >= "2024-01-01") &
    (df["inquiry_date"] < "2024-02-01")
].copy()

result = round(filtered["response_time_hours"].mean())
print(result)


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT inquiry_id, response_time_hours
FROM fct_guest_inquiries
WHERE inquiry_date >= '16-01-2024' AND inquiry_date <= '31-01-2024'
AND response_time_hours > 2

# Ensure inquiry_date is in datetime format if not already
df["inquiry_date"] = pd.to_datetime(df["inquiry_date"])

filtered = df[
    (df["inquiry_date"] >= "2024-01-16") &
    (df["inquiry_date"] <= "2024-01-31") &
    (df["response_time_hours"] > 2)
].copy()

result = filtered[["inquiry_id", "response_time_hours"]]
print(result)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT COUNT(label_id), user_id
FROM email_labels
GROUP BY user_id

result = df.groupby("user_id")["label_id"].count()
print(result)



------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT
  label_id,
  COUNT(email_id) AS totalcount
FROM emails e
GROUP BY label_id
HAVING COUNT(email_id) > 5;

# step 1: get total counts per label
totals = (
    df
    .groupby("label_id")["email_id"]
    .count()
    .reset_index(name="totalcount")
)

	label_id	totalcount
1	B	6
2	C	8
4	E	10



# step 2: keep only those above 5
filtered_totals = totals[totals["totalcount"] > 5]

print(filtered_totals)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT
  el.label_id,
  COUNT(e.email_id) AS email_count
FROM email_labels AS el
LEFT JOIN emails AS e
  ON el.label_id = e.label_id
WHERE el.created_date >= '2024-10-01'
  AND el.created_date <  '2024-11-01'
GROUP BY el.label_id
ORDER BY el.label_id;

import pandas as pd

# 1. Make sure your two DataFrames exist:
#    df_labels: columns → ['label_id', 'user_id', 'created_date', …]
#    df_emails: columns → ['email_id', 'label_id', …]

# 2. Convert created_date to datetime
df_labels['created_date'] = pd.to_datetime(df_labels['created_date'])

# 3. Filter labels created in October 2024
oct_labels = df_labels[
    (df_labels['created_date'] >= '2024-10-01') &
    (df_labels['created_date'] <  '2024-11-01')
].copy()

# 4. LEFT-join emails onto those labels
merged = oct_labels.merge(df_emails, on='label_id', how='left')

# 5. Group and count non-null email_id per label
email_counts = (
    merged
    .groupby('label_id')['email_id']
    .count()                        # only counts non-null entries
    .reset_index(name='email_count')
    .sort_values('label_id')
)

print(email_counts)


-----------------------------------------------------------------------------------------------------------------------------------------------------
SELECT 
  COUNT(DISTINCT customer_id) AS unique_premium_customers
FROM fct_transactions
WHERE 
  service_tier_code LIKE 'PREM%' 
  AND transaction_date >= '2024-04-01'
  AND transaction_date  < '2024-05-01';

import pandas as pd

# ensure your DataFrame df is loaded and has the right dtypes
df['transaction_date'] = pd.to_datetime(df['transaction_date'])

# filter for April 2024 and service tiers starting with "PREM"
filtered = df[
    (df['transaction_date'] >= '2024-04-01') &
    (df['transaction_date'] <  '2024-05-01') &
    (df['service_tier_code'].str.startswith('PREM'))
]

# count distinct customer_id
unique_premium_customers = filtered['customer_id'].nunique()

print(unique_premium_customers)

-----------------------------------------------------------------------------------------------------------------------------------------------------
WITH tier_counts AS (
  SELECT
    service_tier_code,
    COUNT(*) AS transaction_count
  FROM fct_transactions
  WHERE 
    transaction_date >= '2024-05-01'
    AND transaction_date  < '2024-06-01'
  GROUP BY service_tier_code
)
SELECT
  service_tier_code,
  transaction_count,
  RANK() OVER (ORDER BY transaction_count DESC) AS usage_rank
FROM tier_counts
ORDER BY usage_rank;



df['transaction_date'] = pd.to_datetime(df['transaction_date'])

tier_counts = (
    df[(df['transaction_date'] >= '2024-05-01') & (df['transaction_date'] < '2024-06-01')]
    .groupby('service_tier_code')
    .size()
    .reset_index(name='transaction_count')
    .assign(usage_rank=lambda x: x['transaction_count'].rank(method='dense', ascending=False).astype(int))
    .sort_values('usage_rank')
)

print(tier_counts)


-------------------------------------------------------------------------------------------------------------------------------------------------------------
SELECT
  service_tier_code,
  COUNT(transaction_id) AS transactions
FROM fct_transactions
WHERE transaction_date >= '2024-06-01'
  AND transaction_date < '2024-07-01'
GROUP BY service_tier_code
ORDER BY transactions DESC
LIMIT 3;


import pandas as pd

# Ensure 'transaction_date' is datetime
df['transaction_date'] = pd.to_datetime(df['transaction_date'])

# Filter transactions for June 2024
filtered = df[
    (df['transaction_date'] >= '2024-06-01') &
    (df['transaction_date'] <  '2024-07-01') 
]

# Group by service_tier_code, count transaction_id, sort descending, and get top 3
result = (
    filtered.groupby('service_tier_code')['transaction_id']
    .count()
    .reset_index(name='transactions')
    .sort_values('transactions', ascending=False)
    .head(3)
)

print(result)

-------------------------------------------------------------------------------------------------------------------------------------------
SELECT key_message, brand_score
FROM brand_score_metrics
WHERE metric_date >= '2024-10-01' AND 
metric_date < '2025-01-01'


filtered = df[
    (df['metric_date'] >= '2024-10-01') & 
    (df['metric_date'] < '2025-01-01')
]

result = filtered[['key_message', 'brand_score']]

------------------------------------------------------------------------------------------------------------------------------------------------
SELECT key_message, brand_score
FROM brand_score_metrics
WHERE segment_id = 2 AND 
metric_date >= '2024-10-01' AND 
metric_date < '2025-01-01' AND
brand_score >= 90


filtered = df[
    (df['metric_date'] >= '2024-10-01') & 
    (df['metric_date'] < '2025-01-01') &
(df['segment_id'] == 2) & (df['brand_score'] >=90)
]

result = filtered[['key_message', 'brand_score']]



----------------------------------------------------------------------------------------------------------------------------------------------------
#Most recent date should be descending
SELECT key_message,brand_score
FROM brand_score_metrics
WHERE segment_id= 3 AND brand_score > 80
ORDER BY metric_date DESC
LIMIT 5


filtered = (df['segment_id'] == 3 ) & (df['brand_score'] > 80) 
result = filtered [['key_message', 'brand_score']].sort_values('metric_date', ascending=False).head(5) 


result = (
    df[(df['segment_id'] == 3) & (df['brand_score'] > 80)]
    [['key_message', 'brand_score', 'metric_date']]
    .sort_values('metric_date', ascending=False)
    .head(5)
    [['key_message', 'brand_score']]
)


-------------------------------------------------------------------------------------------------------------------------------
SELECT
  COUNT(DISTINCT viewer_id) AS unique_october_viewers
FROM
  viewer_interactions
WHERE
  interaction_date >= '2024-10-01'
  AND interaction_date <  '2024-11-01';


filtered = df[
    (df['metric_date'] >= '2024-10-01') & 
    (df['metric_date'] < '2025-01-01') 

result = filtered['viewer_id'].nunique()
print(result)


-------------------------------------------------------------------------------------------------------------------------------
SELECT viewer_id
FROM viewer_interactions
WHERE interaction_type = 'pause' AND
interaction_date >= '2024-12-01'
    AND interaction_date  < '2025-01-01'



filtered = df[
(df['interaction_date'] >= '2024-12-01') &
(df['interaction_date'] < '2025-01-01')
& df['interaction_type'] == 'pause']

result = filtered['viewer_id']
print(result)

----------------------------------------------------------------------------------------------------------------------------------

WITH jan_loans AS (
  SELECT DISTINCT business_id
  FROM fct_loans
  WHERE loan_issued_date >= '2024-01-01'
    AND loan_issued_date < '2024-02-01'
)

SELECT
  AVG(CASE WHEN jl.business_id IS NOT NULL THEN db.monthly_revenue END) AS avg_revenue_received_loan,
  AVG(CASE WHEN jl.business_id IS NULL THEN db.monthly_revenue END) AS avg_revenue_not_received_loan
FROM dim_businesses db
LEFT JOIN jan_loans jl
  ON db.business_id = jl.business_id
WHERE db.business_size = 'small';



import pandas as pd

# Step 1: Filter January 2024 loans
jan_loans = fct_loans[
    (fct_loans['loan_issued_date'] >= '2024-01-01') &
    (fct_loans['loan_issued_date'] < '2024-02-01')
][['business_id']].drop_duplicates()

# Step 2: LEFT JOIN with dim_businesses
merged = dim_businesses.merge(jan_loans, on='business_id', how='left', indicator=True)

# Step 3: Filter for small businesses only
small_businesses = merged[merged['business_size'] == 'small']

# Step 4: Calculate the two averages
avg_revenue_received_loan = small_businesses[small_businesses['_merge'] == 'both']['monthly_revenue'].mean()
avg_revenue_not_received_loan = small_businesses[small_businesses['_merge'] == 'left_only']['monthly_revenue'].mean()

# Step 5: Print results
print(f"Average monthly revenue for small businesses that received a loan in Jan 2024: {avg_revenue_received_loan:.2f}")
print(f"Average monthly revenue for small businesses that did NOT receive a loan in Jan 2024: {avg_revenue_not_received_loan:.2f}")


-------------------------------------------------------------------------------------------------------------------------------------------------
SELECT
  COUNT(CASE WHEN f.loan_repaid = 1 THEN 1 END) * 100.0 / COUNT(*) AS repayment_percentage
FROM fct_loans f
JOIN dim_businesses d ON f.business_id = d.business_id
WHERE f.loan_issued_date >= '2024-01-01'
  AND f.loan_issued_date < '2024-02-01'
  AND d.business_size = 'small';


import pandas as pd

# Step 1: Filter fct_loans for January 2024
filtered = fct_loans[
    (fct_loans['loan_issued_date'] >= '2024-01-01') &
    (fct_loans['loan_issued_date'] < '2024-02-01')
]

# Step 2: INNER JOIN with dim_businesses on business_id
merged = pd.merge(
    filtered,
    dim_businesses,
    on='business_id',
    how='inner'
)

# Step 3: Filter for small businesses
small_businesses = merged[merged['business_size'] == 'small']

# Step 4: Calculate repayment percentage
repaid_count = (small_businesses['loan_repaid'] == 1).sum()
total_loans = len(small_businesses)

repayment_percentage = (repaid_count / total_loans) * 100 if total_loans > 0 else 0

print(f"Repayment Percentage: {repayment_percentage:.2f}%")


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SELECT
  CASE
    WHEN d.revenue_variability < 0.1 THEN 'Low'
    WHEN d.revenue_variability BETWEEN 0.1 AND 0.3 THEN 'Medium'
    WHEN d.revenue_variability > 0.3 THEN 'High'
  END AS variability_category,
  COUNT(CASE WHEN f.loan_repaid = 1 THEN 1 END) * 100.0 / COUNT(*) AS repayment_percentage
FROM fct_loans f
JOIN dim_businesses d ON f.business_id = d.business_id
WHERE f.loan_issued_date >= '2024-01-01'
  AND f.loan_issued_date < '2024-02-01'
  AND d.business_size = 'small'
GROUP BY variability_category;


--------------------------------------------------------------------------------------------------------------------------------------------
SELECT 
  COUNT(CASE WHEN pickup_count >= 2 THEN 1 END) * 100.0 / COUNT(*) AS percentage_with_multiple_pickups
FROM fct_delivery_routes
WHERE route_date >= '2024-10-01' 
  AND route_date < '2025-01-01';


filtered = fct_loans[
    (fct_loans['loan_issued_date'] >= '2024-10-01') &
    (fct_loans['loan_issued_date'] < '2025-01-01')

countpickup = (filtered['pickup_count'] == 1).sum()
totalcount = len(filtered)
pickup_percentage = (countpickup/totalcount)*100 if totalcount > 0 else 0

------------------------------------------------------------------------------------------------------------------------------------------------
SELECT 
  CASE 
    WHEN pickup_count = 2 THEN 'Only_two'
    WHEN pickup_count >= 3 THEN 'Three_or_more'
  END AS pickup_segment,
  AVG(delivery_time) AS avg_delivery_time
FROM fct_delivery_routes
WHERE route_date >= '2024-10-01' AND route_date < '2025-01-01'
  AND pickup_count >= 2
GROUP BY 
  CASE 
    WHEN pickup_count = 2 THEN 'Only_two'
    WHEN pickup_count >= 3 THEN 'Three_or_more'
  END;




--------------------------------------------------------------------------------------------------------------------------------------
WITH cte AS (
  SELECT 
    COALESCE(earnings, (SELECT AVG(earnings) FROM fct_delivery_routes)) AS earnings_filled,
    pickup_count
  FROM 
    fct_delivery_routes
)
SELECT 
  SUM(earnings_filled) * 1.0 / SUM(pickup_count) AS avg_earnings_per_pickup
FROM 
  cte;

import pandas as pd

# Step 1: Calculate mean earnings, ignoring NaNs
mean_earnings = df["earnings"].mean()

# Step 2: Replace missing (NaN) earnings with the mean
df["earnings_filled"] = df["earnings"].fillna(mean_earnings)

# Step 3: Calculate average earnings per pickup
total_earnings = df["earnings_filled"].sum()
total_pickups = df["pickup_count"].sum()

avg_earnings_per_pickup = total_earnings / total_pickups

print("Average earnings per pickup:", avg_earnings_per_pickup)


-----------------------------------------------------------------------------------------------------------------
SELECT 
    d.component_name,
    COALESCE(COUNT(DISTINCT f.player_id), 0) AS unique_player_count
FROM dim_storyline_components d
LEFT JOIN fct_storyline_interactions f 
    ON d.storyline_component_id = f.storyline_component_id 
    AND f.interaction_date >= '2024-05-01' 
    AND f.interaction_date < '2024-06-01'
GROUP BY d.component_name
ORDER BY d.component_name;


import pandas as pd

# Step 1: Filter interactions to May 2024
filtered_interactions = fct_storyline_interactions[
    (fct_storyline_interactions['interaction_date'] >= '2024-05-01') &
    (fct_storyline_interactions['interaction_date'] < '2024-06-01')
]

# Step 2: Merge with components using LEFT JOIN
merged = dim_storyline_components.merge(
    filtered_interactions,
    on='storyline_component_id',
    how='left'
)

# Step 3: Group by component_name and count unique players (use dropna to ignore components with no interactions)
result = (
    merged.groupby('component_name')['player_id']
    .nunique()
    .reset_index(name='unique_player_count')
    .sort_values('component_name')
)

# Step 4: Replace NaNs with 0 for components that had no interactions
result['unique_player_count'] = result['unique_player_count'].fillna(0).astype(int)

# Display the result
print(result)


---------------------------------------------------------------------------------------------------------------------------------------------------
WITH valid_players AS (
    SELECT 
        player_id
    FROM fct_storyline_interactions
    WHERE interaction_date >= '2024-05-01' 
      AND interaction_date < '2024-06-01'
    GROUP BY player_id
    HAVING COUNT(DISTINCT storyline_component_id) >= 2
)

SELECT 
    d.component_name,
    f.player_id,
    COUNT(f.interaction_id) AS total_interactions
FROM fct_storyline_interactions f
JOIN dim_storyline_components d 
    ON f.storyline_component_id = d.storyline_component_id
JOIN valid_players vp 
    ON f.player_id = vp.player_id
WHERE f.interaction_date >= '2024-05-01' 
  AND f.interaction_date < '2024-06-01'
GROUP BY d.component_name, f.player_id
ORDER BY d.component_name, f.player_id;

------------------------------------------------------------------------------------------------------------------------------------------
filtered = dim_groups[(dim_groups['created_date'] >= '2024-10-01') & # The following tables are loaded as pandas DataFrames with the same names: dim_groups
(dim_groups['created_date'] < '2024-11-01')]
result = filtered['participant_count'].max()
print(result)

----------------------------------------------------------------------------------------------------------------------------------------------
filtered = dim_groups[(dim_groups['created_date'] >= '2024-10-01') & # The following tables are loaded as pandas DataFrames with the same names: dim_groups
(dim_groups['created_date'] < '2024-11-01')]
result = filtered['participant_count'].max()
print(result)

---------------------------------------------------------------------------------------------------------------
filtered = dim_groups[(dim_groups['created_date'] >= '2024-10-01') & # The following tables are loaded as pandas DataFrames with the same names: dim_groups
(dim_groups['created_date'] < '2024-11-01')]
result = filtered['participant_count'].mean()
print(result)
-----------------------------------------------------------------------------------------

SELECT 
    dp.product_category,
    AVG(CAST(fap.clicks AS FLOAT) / NULLIF(fap.impressions, 0) * 100) AS avg_ctr
FROM 
    fct_ad_performance fap
JOIN 
    dim_product dp ON fap.product_id = dp.product_id
WHERE 
    fap.recorded_date >= '2024-10-01' 
    AND fap.recorded_date < '2024-11-01'
    AND dp.product_category ILIKE '%Electronics%'
GROUP BY 
    dp.product_category
ORDER BY 
    avg_ctr DESC;



import pandas as pd

# Step 1: Filter fct_ad_performance to October 2024
ads_oct = fct_ad_performance[
    (fct_ad_performance["recorded_date"] >= '2024-10-01') &
    (fct_ad_performance["recorded_date"] < '2024-11-01')
]

# Step 2: Join with dim_product to get category info
merged = ads_oct.merge(dim_product, on='product_id', how='inner')

# Step 3: Filter only product categories that contain 'Electronics'
electronics_ads = merged[merged["product_category"].str.contains("Electronics", case=False, na=False)]

# Step 4: Calculate CTR per row
electronics_ads["ctr"] = (electronics_ads["clicks"] / electronics_ads["impressions"]) * 100

# Step 5: Compute average CTR per Electronics-related category
result = electronics_ads.groupby("product_category")["ctr"].mean().reset_index()

# Optional: Sort by CTR descending
result = result.sort_values(by="ctr", ascending=False)

# Display
print(result)
------------------------------------------------------------------------------------------------------------------------------------------------
import pandas as pd

# Step 1: Filter fct_ad_performance to October 2024
ads_oct = fct_ad_performance[
    (fct_ad_performance["recorded_date"] >= '2024-10-01') &
    (fct_ad_performance["recorded_date"] < '2024-11-01')
]

# Step 2: Join with dim_product to get category info
merged = ads_oct.merge(dim_product, on='product_id', how='inner')

# Step 3: Calculate CTR per ad (row)
merged["ctr"] = (merged["clicks"] / merged["impressions"]) * 100

# Step 4: Compute average CTR per product category
category_avg_ctr = merged.groupby("product_category")["ctr"].mean().reset_index()

# Step 5: Compute overall average CTR across all ads
overall_avg_ctr = merged["ctr"].mean()

# Step 6: Filter product categories with CTR above overall average
high_performing_categories = category_avg_ctr[category_avg_ctr["ctr"] > overall_avg_ctr]

# Step 7: Sort by CTR descending (optional)
high_performing_categories = high_performing_categories.sort_values(by="ctr", ascending=False)

# Display
print(high_performing_categories)

-----------------------------------------------------------------------------------------------------------------
mask = (data["visit_date"] >= "2024-07-01") & (data["visit_date"] < "2024-08-01")

result = (
    data.loc[mask]
        .groupby(["guest_id", "visit_date", "park_experience_type"])["amount_spent"].mean()
        .groupby("park_experience_type").mean()
        .reindex(data["park_experience_type"].unique(), fill_value=0.0)
        .reset_index(name="avg_spent")
)

print(result)


------------------------------------------------------------------------------------------------------------------------
SELECT 
    SUM(f.quantity_sold) AS Total_Sales_Volume
FROM fct_sales f
INNER JOIN dim_products d
    ON f.product_id = d.product_id
WHERE d.category = 'Essential Household'
  AND f.sale_date >= '2024-07-01'
  AND f.sale_date < '2024-08-01';


import pandas as pd

# Step 1: Merge sales with product info
merged = fct_Sales.merge(
    dim_products,
    on='product_id',
    how='inner'
)

# Step 2: Filter to Essential Household and July 2024
filtered_interactions = merged[
    (merged['category'] == 'Essential Household') &
    (merged['sale_date'] >= '2024-07-01') &
    (merged['sale_date'] < '2024-08-01')
]

# Step 3: Compute total sales volume
result = filtered_interactions['quantity_sold'].sum()

print("Total_Sales_Volume:", result)



-----------------------------------------------------------------------------------------------------------------------------
SELECT 
    d.product_id,
    d.product_name,
    CASE 
        WHEN AVG(f.unit_price) < 5 THEN 'Low'
        WHEN AVG(f.unit_price) BETWEEN 5 AND 15 THEN 'Medium'
        WHEN AVG(f.unit_price) > 15 THEN 'High'
    END AS Category_average
FROM fct_sales f
INNER JOIN dim_products d
    ON f.product_id = d.product_id
WHERE d.category = 'Essential Household'
  AND f.sale_date >= '2024-07-01'
  AND f.sale_date < '2024-08-01'
GROUP BY d.product_id, d.product_name;



------------------------------------------------------------------------------------------------------------------------------------------
SELECT 
    CASE 
        WHEN f.unit_price < 5 THEN 'Low'
        WHEN f.unit_price BETWEEN 5 AND 15 THEN 'Medium'
        WHEN f.unit_price > 15 THEN 'High'
    END AS Price_Range,
    SUM(f.quantity_sold * f.unit_price) AS Total_Sales_Volume
FROM fct_sales f
INNER JOIN dim_products d
    ON f.product_id = d.product_id
WHERE d.category = 'Essential Household'
  AND f.sale_date >= '2024-07-01'
  AND f.sale_date < '2024-08-01'
GROUP BY 
    CASE 
        WHEN f.unit_price < 5 THEN 'Low'
        WHEN f.unit_price BETWEEN 5 AND 15 THEN 'Medium'
        WHEN f.unit_price > 15 THEN 'High'
    END
ORDER BY Total_Sales_Volume DESC
LIMIT 1;




---------------------------------------------------------------------------------------------------------------------------------
SELECT COUNT(DISTINCT u.user_id) AS unique_users
FROM user_streams u
INNER JOIN artist_recommendations a
ON u.user_id = a.user_id
AND u.artist_id = a.artist_id
WHERE stream_date >= recommendation_date


import pandas as pd

# Merge on both user_id and artist_id
merged = user_streams.merge(
    artist_recommendations,
    on=['user_id', 'artist_id'],
    how='inner'
)

# Keep only streams that happened on or after recommendation_date
filtered = merged[merged['stream_date'] >= merged['recommendation_date']]

# Count unique users
result = filtered['user_id'].nunique()

print(result)


------------------------------------------------------------------------------------------------------
SELECT 
    AVG(stream_count) AS avg_streams_per_recommended_artist
FROM (
    SELECT 
        us.user_id,
        us.artist_id,
        COUNT(*) AS stream_count
    FROM user_streams us
    INNER JOIN artist_recommendations ar
        ON us.user_id = ar.user_id
       AND us.artist_id = ar.artist_id
    WHERE us.stream_date >= ar.recommendation_date
      AND us.stream_date >= '2024-05-01'
      AND us.stream_date < '2024-06-01'
    GROUP BY us.user_id, us.artist_id
) t;



import pandas as pd

# Merge on both user_id and artist_id
merged = user_streams.merge(
    artist_recommendations,
    on=['user_id', 'artist_id'],
    how='inner'
)

# Filter streams that happened on or after recommendation_date
filtered = merged[
    (merged['stream_date'] >= merged['recommendation_date']) &
    (merged['stream_date'] >= '2024-05-01') &
    (merged['stream_date'] < '2024-06-01')
]

# Count streams per (user_id, artist_id)
grouped = filtered.groupby(['user_id', 'artist_id']).size().reset_index(name='stream_count')

# Average across recommended artists
avg_streams_per_recommended_artist = grouped['stream_count'].mean()

print(avg_streams_per_recommended_artist)

-------------------------------------------------------------------------------------------------------------------------
with cte AS (
SELECT COUNT(DISTINCT u.artist_id) as artist_recommendations
FROM user_streams u
INNER JOIN artist_recommendations a
ON u.user_id = a.user_id
AND u.artist_id = a.artist_id
WHERE u.stream_date>=a.recommendation_date AND u.artist_id IS NOT NULL
GROUP BY u.user_id)

SELECT AVG(artist_recommendations)
FROM cte



import pandas as pd

# Merge
merged = user_streams.merge(
    artist_recommendations,
    on=["user_id", "artist_id"],
    how="inner"
)
# Filter condition
filtered = merged[
    (merged["stream_date"] >= merged["recommendation_date"]) &
    (merged["artist_id"].notnull())
]
# Group by user, count distinct artist_id, then take average
cte = filtered.groupby("user_id")["artist_id"].nunique()
result = cte.mean()
print(result)

-------------------------------------------------------------------------------------------------------------------------------------------------
with cte AS (
SELECT f.click_id,f.user_id,f.event_id,f.click_date,u.preferred_category,
e.category_name, CASE WHEN u.preferred_category=e.category_name THEN 'Yes' 
  ELSE 'No' END AS Label
FROM fct_event_clicks f
INNER JOIN dim_events e
ON f.event_id = e.event_id
INNER JOIN dim_users u
ON f.user_id = u.user_id
WHERE f.click_date >= '2024-03-01' 
  AND f.click_date < '2024-04-01')

SELECT user_id, category_name, label
FROM cte





import pandas as pd
import numpy as np

# 1. Join tables
merged = (
    fct_event_clicks
    .merge(dim_events, on="event_id", how="inner")
    .merge(dim_users, on="user_id", how="inner")
)

# 2. Ensure click_date is datetime
merged["click_date"] = pd.to_datetime(merged["click_date"])

# 3. Filter by date range
filtered = merged[
    (merged["click_date"] >= "2024-03-01") &
    (merged["click_date"] < "2024-04-01")
]

# 4. Add computed column (CASE WHEN … THEN … ELSE …)
filtered["Label"] = np.where(
    filtered["preferred_category"] == filtered["category_name"],
    "Yes",
    "No"
)

# 5. Select final columns
result = filtered[["user_id", "category_name", "Label"]]

--------------------------------------------------------------------------------------------------------------------------------------
import sys
import math

# Auto-generated code below aims at helping you parse
# the standard input according to the problem statement.

n = int(input())
k = int(input())

# Write an answer using print
# To debug: print("Debug messages...", file=sys.stderr, flush=True)

print(int(math.factorial(n)/(math.factorial(n-k)*math.factorial(k))))


-----------------------------------------------------------------------------------------------------
n = int(input())
for i in range(n):
    s = input()
    result = ""
    for char in s:
        if char not in result:
            result += char
    print(result)

-------------------------------------------------------------------------------------------------------------
import pandas as pd

# Example: load data (replace this with your actual data source)
# fct_guest_spending = pd.read_csv("fct_guest_spending.csv")

# --- Step 1: Filter for September 2024 ---
fct_guest_spending['visit_date'] = pd.to_datetime(fct_guest_spending['visit_date'])
sept_data = fct_guest_spending[
    (fct_guest_spending['visit_date'].dt.year == 2024) &
    (fct_guest_spending['visit_date'].dt.month == 9)
]

# --- Step 2: Aggregate total spending per guest ---
guest_spending = sept_data.groupby('guest_id', as_index=False)['amount_spent'].sum()
guest_spending.rename(columns={'amount_spent': 'total_spent'}, inplace=True)

# --- Step 3: Exclude guests with no purchases (0 or NaN) ---
guest_spending = guest_spending[guest_spending['total_spent'] > 0]

# --- Step 4: Categorize spending ---
def categorize_spending(amount):
    if amount < 50:
        return "Low"
    elif amount < 100:
        return "Medium"
    else:
        return "High"

guest_spending['spending_segment'] = guest_spending['total_spent'].apply(categorize_spending)

# --- Step 5: View results ---
print(guest_spending.head())


SELECT 
    guest_id,
    SUM(amount_spent) AS total_spent,
    CASE 
        WHEN SUM(amount_spent) < 50 THEN 'Low'
        WHEN SUM(amount_spent) < 100 THEN 'Medium'
        ELSE 'High'
    END AS spending_segment
FROM fct_guest_spending
WHERE 
    visit_date >= '2024-09-01'
    AND visit_date < '2024-10-01'
GROUP BY guest_id
HAVING SUM(amount_spent) > 0
ORDER BY total_spent DESC;
--------------------------------------------------------------------------------------------------------------------------------------------------------
import pandas as pd

# Ensure 'sale_date' is datetime
fct_sales["sale_date"] = pd.to_datetime(fct_sales["sale_date"])

# Apply the filters
filtered_sales = fct_sales[
    (fct_sales["sale_date"] >= "2025-01-01") &
    (fct_sales["sale_date"] < "2025-04-01") &
    (fct_sales["sale_amount"].isna())
]

# Display the filtered records
print(filtered_sales)

#UNIQUE VALUES FOR "celebrity_id", "product_id"
filtered_sales = fct_sales[
    (fct_sales["sale_date"] >= "2025-01-01") &
    (fct_sales["sale_date"] < "2025-04-01")
]

# Keep only the first row of each unique (celebrity_id, product_id) pair
filtered_sales = filtered_sales.drop_duplicates(subset=["celebrity_id", "product_id"])

print(filtered_sales)

---------------------------------------------------------------------------------------
filtered_sales = fct_sales[
    (fct_sales["sale_date"] >= "2025-01-01") &
    (fct_sales["sale_date"] < "2025-04-01")
]

result = (
    filtered_sales
    .groupby("celebrity_id")["sale_amount"]
    .sum()
    .sort_values(ascending=False)
)
result

--------------------------------------------------------------------------------------------------
# Convert and filter
fct_transactions["transaction_date"] = pd.to_datetime(fct_transactions["transaction_date"])
df = fct_transactions[
    (fct_transactions["transaction_date"] >= "2024-10-01") &
    (fct_transactions["transaction_date"] < "2025-01-01")
]

# Group by user and compute stats
g = df.groupby("user_id")["transaction_date"].agg(['min', 'max', 'count'])
g["days_diff"] = (g["max"] - g["min"]).dt.days

# Averages
avg_2 = g[g["count"] == 2]["days_diff"].mean()
avg_3_plus = g[g["count"] >= 3]["days_diff"].mean()

print(avg_2, avg_3_plus)

-----------------------------------------------------------------------------------------------------------
import pandas as pd

# Filter rows between dates
data = fct_transactions[
    (fct_transactions["transaction_date"] >= "2025-04-01") &
    (fct_transactions["transaction_date"] < "2025-07-01")
]

# Group by payment_method and count transaction_id
data = data.groupby("payment_method")["transaction_id"].count()

print(data)
